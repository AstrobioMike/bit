#!/usr/bin/env python

import argparse
import re
import sys
import os
import subprocess
from pathlib import Path
import pysam # type: ignore
import pandas as pd
import numpy as np
import math
from Bio import SeqIO # type: ignore
from Bio.Seq import Seq
from Bio.SeqRecord import SeqRecord
import matplotlib.pyplot as plt # type: ignore
from colorama import Fore, Style, init
init(autoreset=True)

desc = """
       This script identifies and pulls out regions of relatively higher and lower coverage when
       given a reference fasta and a bam file. It generates a bed file of the specified window-
       and step-size, utilizes mosdepth to get the coverage of those windows, then generates stats
       for those windows and pulls out regions with coverage above and below specified thresholds.
       It outputs a table of coverage stats for all windows, a table of merged adjacent windows
       ("regions"), and identified regions in fasta format. By default it looks for coverage variations
       from the global mean coverage, but you can tell it to use per-contig coverages by adding the
       `--per-contig` flag. Additionally, it is recommended to exclude contigs holding mitochondrial
       genomes or chloroplasts due to their generally very relative high coverage, unless you
       specifically want to investigate them too (if so, probably use `--per-contig` mode). For
       version info, run `bit-version`.
       """

parser = argparse.ArgumentParser(
    description=desc,
    epilog="Ex. usage: bit-cov-analyzer -r reference.fasta -b mapping.bam",
)
required = parser.add_argument_group("REQUIRED PARAMETERS")
optional = parser.add_argument_group("OPTIONAL PARAMETERS")

required.add_argument(
    "-r",
    "--reference-fasta",
    metavar="<FILE>",
    help='reference fasta file',
    required=True,
)
required.add_argument(
    "-b",
    "--bam-file",
    metavar="<FILE>",
    help="bam file",
    required=True,
)
optional.add_argument(
    "-o",
    "--output-dir",
    metavar="<STR>",
    help='directory for the output files (default: "cov-analyzer/")',
    default="cov-analyzer",
)
optional.add_argument(
    "-H",
    "--high-threshold",
    metavar="<FLOAT>",
    help='high threshold for regions-of-interest (default: 5; e.g., 5 would\
          identify regions with 5-fold+ higher coverage than the mean of the whole ref)',
    type=float,
    default=5,
)
optional.add_argument(
    "-L",
    "--low-threshold",
    metavar="<FLOAT>",
    help='low threshold for regions-of-interest (default: 5; e.g., 5 would\
          identify regions with 5-fold+ lower coverage than the mean of the whole ref)',
    type=float,
    default=5,
)
optional.add_argument(
    "-e",
    "--exclude-contigs",
    metavar="<STR>",
    nargs="+",
    help='space-delimited list of contigs to exclude from analysis (e.g., "-e NC_000932.1 NC_037304.1"; default: None)',
    type=str,
    default=[],
)
optional.add_argument(
    "--per-contig",
    action="store_true",
    help='run the analysis using per-contig mean-coverages for each contig to find high/low regions, instead of using the mean coverage of the whole reference (default: False)',
    default=False,
)
optional.add_argument(
    "-s",
    "--sliding-window-size",
    metavar="<INT>",
    help='sliding window size (default: 50)',
    type=int,
    default=50,
)
optional.add_argument(
    "-S",
    "--step-size",
    metavar="<INT>",
    help='step size for sliding window (default: 10)',
    type=int,
    default=10,
)
optional.add_argument(
    "-g",
    "--allowed-gap",
    metavar="<INT>",
    help='number of bases allowed between qualifying windows (those with coverages above/below the specified thresholds) to still merge them into one contiguous region (default: 1000)',
    type=int,
    default=1000,
)
optional.add_argument(
    "-B",
    "--buffer",
    metavar="<INT>",
    help='add this length to each side of a region of interest when pulled out as fasta (default: 100)',
    type=int,
    default=100,
)


###############################################################################

def main(reference_fasta, bam_file, output_dir,
         high_threshold, low_threshold, exclude_contigs: list,
         per_contig, sliding_window_size, step_size, allowed_gap, buffer):

    preflight_checks(reference_fasta, bam_file, output_dir, exclude_contigs)

    contig_lengths = get_contig_lengths(reference_fasta)

    window_bed_path = generate_sliding_bed_file(reference_fasta,
                                                sliding_window_size,
                                                step_size,
                                                output_dir)

    mosdepth_regions_file = run_mosdepth(bam_file, window_bed_path, output_dir)

    cov_df = read_mosdepth_regions_file(mosdepth_regions_file)
    if exclude_contigs:
        cov_df = cov_df.loc[~cov_df["contig"].isin(exclude_contigs)].reset_index(drop=True)

    cov_stats = CoverageStats(cov_df, per_contig)

    high_merged_regions = cov_stats.merge_windows(type="high", threshold = high_threshold, contig_lengths = contig_lengths, allowed_gap = allowed_gap)
    low_merged_regions = cov_stats.merge_windows(type="low", threshold = 1/low_threshold, contig_lengths = contig_lengths, allowed_gap = allowed_gap)

    generate_outputs(reference_fasta, high_merged_regions, low_merged_regions,
                     cov_df, cov_stats, buffer, output_dir, contig_lengths)

###############################################################################


def preflight_checks(reference_fasta, bam_file, output_dir, exclude_contigs):

    paths_list = [reference_fasta, bam_file]
    check_files_are_found(paths_list)
    check_bam_file_is_indexed(bam_file)
    check_fasta_file_is_indexed(reference_fasta)
    check_excluded_contigs(reference_fasta, exclude_contigs)
    os.makedirs(f"{output_dir}/mosdepth-files", exist_ok=True)


    full_cmd = " ".join(arg for arg in sys.argv)
    with open(f"{output_dir}/runlog.txt", "w") as out:

        out.write("Command executed:\n")
        out.write(f"  {full_cmd}\n\n")


def check_files_are_found(paths_list):
    for path in paths_list:
        if not Path(path).is_file():
            print(f"\n    We were not able to find the input file: {path}")
            notify_premature_exit()


def notify_premature_exit():
    print("\n    Exiting for now :(\n")
    sys.exit(1)


def check_bam_file_is_indexed(bam_file):
    if not Path(bam_file + ".bai").is_file():
        cmd = f"samtools index {bam_file}"
        subprocess.run(cmd, shell=True)


def check_fasta_file_is_indexed(reference_fasta):
    if not Path(reference_fasta + ".fai").is_file():
        cmd = f"samtools faidx {reference_fasta}"
        subprocess.run(cmd, shell=True)


def check_excluded_contigs(reference_fasta, exclude_contigs):
    if exclude_contigs:
        with pysam.FastaFile(reference_fasta) as fasta:
            ref_contigs = set(fasta.references)
        missing_contigs = [contig for contig in exclude_contigs if contig not in ref_contigs]
        if missing_contigs:
            print(f"\n    The following contigs were specified for exclusion but were not found in the reference fasta:\n")
            for contig in missing_contigs:
                print(f"        {contig}")
            print("\n    Please check the contig names and try again.")
            notify_premature_exit()


def get_contig_lengths(reference_fasta):
    with pysam.FastaFile(reference_fasta) as fasta:
        return dict(zip(fasta.references, fasta.lengths))

def generate_sliding_bed_file(reference_fasta, sliding_window_size, step_size, output_dir):
    bed_outfile = f"{output_dir}/mosdepth-files/sliding-windows.bed"
    with pysam.FastaFile(reference_fasta) as fasta, open(bed_outfile, "w") as bed_out:
        for contig, length in zip(fasta.references, fasta.lengths):
            for start in range(0, length - sliding_window_size + 1, step_size):
                end = start + sliding_window_size
                bed_out.write(f"{contig}\t{start}\t{end}\n")

    return bed_outfile


def run_mosdepth(bam_file, window_bed_path, output_dir):
    mosdepth_out_prefix = f"{output_dir}/mosdepth-files/{bam_file[:-4]}"
    cmd = f"mosdepth --by {window_bed_path} {mosdepth_out_prefix} {bam_file}"
    subprocess.run(cmd, shell=True)
    mosdepth_regions_file = f"{mosdepth_out_prefix}.regions.bed.gz"

    return mosdepth_regions_file


def read_mosdepth_regions_file(mosdepth_regions_file):
    cov_df = pd.read_csv(
        mosdepth_regions_file,
        sep="\t",
        header=None,
        names=["contig", "start", "end", "cov"],
        compression="gzip",
    ).astype({"contig": str, "start": int, "end": int, "cov": float})

    return cov_df


class CoverageStats:
    def __init__(self, df, per_contig = False):
        self.df = df.copy()
        self.per_contig = per_contig

        # global stats
        self.global_mean = self.df["cov"].mean()
        self.global_std  = self.df["cov"].std()
        self.global_median = self.df["cov"].median()
        self.global_min = self.df["cov"].min()
        self.global_max = self.df["cov"].max()
        self.df['percentile'] = self.df['cov'].rank(pct=True) * 100

        # per-contig stats
        group = self.df.groupby("contig")["cov"]
        self.df["contig_mean"] = group.transform("mean")
        self.df["contig_std"] = group.transform("std")
        self.df["contig_median"] = group.transform("median")
        self.df["contig_min"] = group.transform("min")
        self.df["contig_max"] = group.transform("max")
        self.df["contig_percentile"] = group.transform(lambda x: x.rank(pct=True) * 100)

        if per_contig:
            self.df["baseline_mean"] = self.df["contig_mean"]
            self.df["baseline_std"] = self.df["contig_std"]
            self.df["percentile"] = self.df["contig_percentile"]
        else:
            self.df["baseline_mean"] = self.global_mean
            self.df["baseline_std"] = self.global_std
            self.df["percentile"] = self.df["percentile"]

        # fold diff, zscore, log2-fold diff
        nobody_likes_zero = 1e-6
        self.df["fold_diff"] = self.df["cov"] / self.df["baseline_mean"]
        self.df["signed_fold_diff"] = self.df["fold_diff"].where(
            self.df["fold_diff"] >= 1,
            -1.0 / self.df["fold_diff"]
        )
        self.df["zscore"] = (self.df["cov"] - self.df["baseline_mean"]) / self.df["baseline_std"]
        self.df["log2_fold_diff"] = np.log2((self.df["cov"] + nobody_likes_zero) / (self.df["baseline_mean"] + nobody_likes_zero))


    def global_percentile(self, p):
        return np.percentile(self.df["cov"], p)


    def get_windows(self, type = "high", method = "fold", threshold = 10):
        """
        Filter for windows above or below a given threshold.

        methods: "percentile", "zscore", "fold", or "log2fold"
        """
        if method == "percentile":
            metric = self.df["percentile"]
        elif method == "zscore":
            metric = self.df["zscore"]
        elif method == "fold":
            metric = self.df["fold_diff"]
        elif method == "log2fold":
            metric = self.df["log2_fold_diff"]
        else:
            raise ValueError(f"Nope nope nope: {method!r}")

        # filter and return a new dataframe
        if type == "low":
            mask = (metric <= threshold) | (self.df["cov"] == 0)
        elif type == "high":
            mask = (metric >= threshold)
        else:
            raise ValueError(f"Nope nope nope: {type!r}")

        return self.df.loc[mask].reset_index(drop=True)


    def merge_windows(self, *, type = "high", method = "fold", threshold = 10, edge_pad = 200, contig_lengths = None, allowed_gap = 0):

        df = self.get_windows(type=type, method=method, threshold=threshold)

        # here, if type == "low", we are filtering out windows that are within <edge_pad> bases of the start/end of a contig
        if type =="low":
            df["contig_length"] = df["contig"].map(contig_lengths)
            df = df[(df.start >= edge_pad) & (df.end <= df.contig_length - edge_pad)].drop(columns="contig_length")

        regions = []

        for contig, group in df.sort_values(["contig", "start"]).groupby("contig", sort=False):
            curr_start = curr_end = None
            covs = []
            # iterating through each window
            for row in group.itertuples(index = False):
                start, end, cov = row.start, row.end, row.cov
                if curr_start is None:
                    # starting a new region
                    curr_start, curr_end = start, end
                    covs = [cov]
                elif start <= curr_end + allowed_gap:
                    # adjacent, or within allowed_gap range, so extending region
                    curr_end = max(curr_end, end)
                    covs.append(cov)
                else:
                    # not adjacent, so saving the region and starting a new one
                    regions.append(self._summarize_region(contig, curr_start, curr_end, covs))
                    curr_start, curr_end = start, end
                    covs = [cov]

            if curr_start is not None:
                # saving the last region
                regions.append(self._summarize_region(contig, curr_start, curr_end, covs))

        return pd.DataFrame(regions,
                            columns=["contig","start","end", "length",
                                     "cov", "percentile", "zscore", "signed_fold_diff",
                                     "fold_diff", "log2_fold_diff"])


    def _summarize_region(self, contig, start, end, covs):
        "helper to compute the summary metrics given a list of window coverages"
        region_length = end - start
        region_mean_cov = float(np.mean(covs))

        if self.per_contig:
            subset = self.df.loc[self.df["contig"] == contig, "cov"]
            percentile = float((subset <= region_mean_cov).mean() * 100)
            baseline_mean = float(self.df.loc[self.df["contig"] == contig, "contig_mean"].iloc[0])
            baseline_std = float(self.df.loc[self.df["contig"] == contig, "contig_std"].iloc[0])
        else:
            percentile = float((self.df["cov"] <= region_mean_cov).mean() * 100)
            baseline_mean = self.global_mean
            baseline_std = self.global_std

        # computing metrics based on appropriate baseline
        fold_diff = region_mean_cov / baseline_mean
        signed_fold_diff = fold_diff if fold_diff >= 1 else -1.0 / fold_diff
        zscore = (region_mean_cov - baseline_mean) / baseline_std
        nobody_likes_zero = 1e-6
        log2_fold_diff = float(np.log2((region_mean_cov + nobody_likes_zero) / (baseline_mean + nobody_likes_zero)))

        return (contig, start, end, region_length, region_mean_cov, percentile, zscore, signed_fold_diff, fold_diff, log2_fold_diff)


def generate_outputs(reference_fasta, high_merged_regions, low_merged_regions,
                     cov_df, cov_stats, buffer, output_dir, contig_lengths):

    print()
    write_window_cov_stats(cov_stats, output_dir, contig_lengths)
    write_windows_table(cov_stats, output_dir)
    write_window_plot_cov_histogram(cov_df, output_dir)

    write_regions_of_interest_table(high_merged_regions, output_dir, "high")
    write_regions_fasta(reference_fasta, high_merged_regions, buffer, output_dir, "high", contig_lengths)

    write_regions_of_interest_table(low_merged_regions, output_dir, "low")
    write_regions_fasta(reference_fasta, low_merged_regions, buffer, output_dir, "low", contig_lengths)
    print()


def write_window_cov_stats(cov_stats, output_dir, contig_lengths):
    out_txt = f"{output_dir}/window-coverage-overview.txt"
    out_tsv = f"{output_dir}/window-coverage-overview.tsv"
    percentiles = [0.01, 0.1, 1, 5, 10, 25, 50, 75, 90, 95, 99, 99.9, 99.99]

    rows = []

    # assembly‐level
    assembly_length = sum(contig_lengths.values())
    rows.append({
        "name":        "full-assembly",
        "length_bp":   assembly_length,
        "num_windows": len(cov_stats.df),
        "mean":        cov_stats.global_mean,
        "median":      cov_stats.global_median,
        "std":         cov_stats.global_std,
        "min":         cov_stats.global_min,
        "max":         cov_stats.global_max,
        **{f"p{p}": cov_stats.df["cov"].quantile(p/100) for p in percentiles}
    })

    # per-contig
    for contig, group in cov_stats.df.groupby("contig", sort=False):
        contig_len = contig_lengths.get(contig, 0)
        rows.append({
            "name":        contig,
            "length_bp":   contig_len,
            "num_windows": len(group),
            "mean":        group["cov"].mean(),
            "median":      group["cov"].median(),
            "std":         group["cov"].std(),
            "min":         group["cov"].min(),
            "max":         group["cov"].max(),
            **{f"p{p}": group["cov"].quantile(p/100) for p in percentiles}
        })

    # writing assembly level
    with open(out_txt, "w") as out:
        out.write("====================================\n")
        out.write("=== GLOBAL WINDOW COVERAGE STATS ===\n")
        out.write("====================================\n")
        out.write(f"Assembly size: {assembly_length:,} bp\n")
        out.write(f"Number of windows: {len(cov_stats.df)}\n")
        out.write(f"Mean:   {cov_stats.global_mean:.2f}\n")
        out.write(f"Median: {cov_stats.global_median:.2f}\n")
        out.write(f"Std:    {cov_stats.global_std:.2f}\n")
        out.write(f"Min:    {cov_stats.global_min:.2f}\n")
        out.write(f"Max:    {cov_stats.global_max:.2f}\n\n")

        out.write("Percentiles (global):\n")
        for p in percentiles:
            val = cov_stats.df["cov"].quantile(p/100)
            out.write(f"  {p:>6.2f}% : {val:.2f}\n")

        # now writing per-contig
        out.write("\n========================================\n")
        out.write("=== PER-CONTIG WINDOW COVERAGE STATS ===\n")
        out.write("========================================\n")
        for contig, group in cov_stats.df.groupby("contig", sort=False):
            out.write(f"\n-- {contig} --\n")
            contig_length = contig_lengths.get(contig, 0)
            gm = group["cov"].mean()
            md = group["cov"].median()
            sd = group["cov"].std()
            mn = group["cov"].min()
            mx = group["cov"].max()
            out.write(f"Contig size: {contig_length:,} bp\n")
            out.write(f"Number of windows: {len(group)}\n")
            out.write(f"Mean:   {gm:.2f}\n")
            out.write(f"Median: {md:.2f}\n")
            out.write(f"Std:    {sd:.2f}\n")
            out.write(f"Min:    {mn:.2f}\n")
            out.write(f"Max:    {mx:.2f}\n")
            out.write("\nPercentiles:\n")
            for p in percentiles:
                val = group["cov"].quantile(p/100)
                out.write(f"  {p:>6.2f}% : {val:.2f}\n")
            out.write("\n")

    tee(f"  Window-coverage summary written to:\n      {Fore.YELLOW}{out_txt}", log_file)

    # writing out as tsv
    df_summary = pd.DataFrame(rows)
    cols = ["name","length_bp","num_windows","mean","median","std","min","max"] \
           + [f"p{p}" for p in percentiles]
    df_summary.to_csv(out_tsv, sep="\t", index=False, float_format="%.2f", columns=cols)

    tee(f"  Window-coverage summary table written to:\n      {Fore.YELLOW}{out_tsv}", log_file)


def tee(msg, log_path, end="\n"):
    ANSI_ESCAPE = re.compile(r'\x1B\[[0-?]*[ -/]*[@-~]')
    print(msg, end=end, file=sys.stdout)
    clean = ANSI_ESCAPE.sub('', msg)
    with open(log_path, "a") as f:
        f.write(clean + end)


def write_windows_table(cov_stats, output_dir):
    out_path = f"{output_dir}/window-coverage-stats.tsv.gz"
    cov_stats.df.to_csv(out_path, sep="\t", index=False, float_format="%.2f", compression="gzip",
                        columns=["contig", "start", "end",
                                 "cov", "percentile", "zscore", "signed_fold_diff",
                                 "fold_diff", "log2_fold_diff"])
    tee(f"  Window-coverage stats table written to:\n      {Fore.YELLOW + out_path}", log_file)


def write_window_plot_cov_histogram(cov_df, output_dir):
    plt.figure()
    plt.hist(np.log10(cov_df["cov"] + 1), bins=100, density=True)
    plt.xlabel("log₁₀(Coverage + 1)")
    plt.ylabel("Frequency")
    plt.title("Distribution of Window Coverages (log₁₀)")
    mean_cov = cov_df["cov"].mean()
    median_cov = cov_df["cov"].median()
    num_windows = len(cov_df)
    ax = plt.gca()
    ax.text(0.95, 0.95,
            f"N: {num_windows:,}\nMean: {mean_cov:.2f}\nMedian: {median_cov:.2f}",
            transform=ax.transAxes,
            fontsize=10,
            va="top",
            ha="right",
            bbox=dict(boxstyle="round,pad=0.3", edgecolor="black", facecolor="white"))
    out_path = f"{output_dir}/window-coverage-histogram.png"
    plt.savefig(out_path, dpi=125, bbox_inches="tight")
    plt.close()
    tee(f"  Window-coverage histogram written to:\n      {Fore.YELLOW + out_path}", log_file)


def write_regions_of_interest_table(merged_regions, output_dir, type):
    out_path = f"{output_dir}/{type}-coverage-regions-of-interest.tsv"

    sorted_df = merged_regions.sort_values(by=["contig", "start"], ascending=[True, True])

    sorted_df.to_csv(out_path, sep="\t", index=False, float_format="%.2f")

    if len(sorted_df) > 0:
        tee(f"\n  Number of {type}-coverage regions identified: {Fore.YELLOW + str(len(sorted_df))}", log_file)
        tee(f"    {type.capitalize()}-coverage regions-of-interest table written to:\n      {Fore.YELLOW + out_path}", log_file)
    else:
        tee(f"\n  No {type}-coverage regions-of-interest identified.", log_file)


def write_regions_fasta(reference_fasta, regions_df, buffer, output_dir, type, contig_lengths):
    output_fasta = f"{output_dir}/{type}-coverage-regions-of-interest.fasta"
    fasta = pysam.FastaFile(reference_fasta)
    records = []

    for _, row in regions_df.iterrows():
        contig = row["contig"]
        orig_start = row["start"]
        orig_end = row["end"]
        cov = row["cov"]

        # adding the buffer to each side of the region
        start_buf = max(0, orig_start - buffer)
        end_buf = min(contig_lengths[contig], orig_end + buffer)

        # getting the seq
        seq = fasta.fetch(contig, start_buf, end_buf)

        # having header include original and buffered coordinates
        header = (f"{contig}:{start_buf}-{end_buf}"
            f"_orig_{orig_start}-{orig_end}"
            f"_cov_{cov:.2f}")

        rec = SeqRecord(Seq(seq),
                        id=header,
                        description="")
        records.append(rec)
    fasta.close()

    if len(records) > 0:
        with open(output_fasta, "w") as out:
            SeqIO.write(records, out, "fasta")

        print(f"    {type.capitalize()}-coverage regions-of-interest fasta written to:\n      {Fore.YELLOW + output_fasta}")


if __name__ == "__main__":
    if len(sys.argv) == 1:  # pragma: no cover
        parser.print_help(sys.stderr)
        sys.exit(0)
    args = parser.parse_args()

    log_file = f"{args.output_dir}/runlog.txt"

    main(
        reference_fasta=args.reference_fasta,
        bam_file=args.bam_file,
        output_dir=args.output_dir,
        high_threshold=args.high_threshold,
        low_threshold=args.low_threshold,
        exclude_contigs=args.exclude_contigs,
        per_contig=args.per_contig,
        sliding_window_size=args.sliding_window_size,
        step_size=args.step_size,
        allowed_gap=args.allowed_gap,
        buffer=args.buffer,
    )
